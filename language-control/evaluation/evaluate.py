from typing import List
import re
import logging
from datasets import load_dataset
import json

def extract_eval_results(generation_result: str):
    """Extracts the evaluation score from the LLM judge generation result.

    Args:
        generation_result (str): The text generated by the LLM judge.

    Returns:
        int: The evaluation score extracted from the generation result.
    """
    logging.info("Extracting evaluation score from the generation result.")
    match = re.search(r'{"answer": (\d+)}', generation_result)
    if match:
        return int(match.group(1))
    else:
        logging.info("Could not extract the evaluation score from the generation result. Setting the score to 0")
        return 0

def get_model_generation(client, prompt: str, model_name: str):
    """Gets the model generation from the Mistral API.

    Args:
        client (mistralai.Mistral): The Mistral client.
        prompt (str): The prompt for text generation.
        model_name (str): The name of the model to use for generation.

    Returns:
        str: The generated text.
    """
    logging.info("Getting the model generation from the Mistral API.")
    chat_response = client.chat.complete(
        model=model_name,
        messages=[
            {
                "role": "user",
                "content": prompt
            },
        ],
        top_p=0.2,
        max_tokens=2000
    )
    generation_result = chat_response.choices[0].message.content
    return generation_result

def download_truthfulqa():
    """Downloads the TruthfulQA dataset from HuggingFace.

    Returns:
        dataset: The TruthfulQA dataset.
    """
    logging.info("Downloading the TruthfulQA dataset from HuggingFace to get the question and correct answers.")
    truthfulqa = load_dataset("truthfulqa/truthful_qa", name="generation", split="validation")
    return truthfulqa

def load_prompt_template(prompt_path: str):
    """Loads the prompt template from the given path.

    Args:
        prompt_path (str): The path to the prompt template.

    Returns:
        str: The prompt template.
    """
    logging.info(f"Loading the prompt template from {prompt_path}.")
    with open(prompt_path, "r") as f:
        prompt_template = f.read()
    return prompt_template

def evaluate_model_generations(answers: List[str], fluency_prompt_path: str, correctness_prompt_path: str, model_name: str, language: str, finnish_questions_path: str, client):
    """Evaluates the model generations using the LLM judge.

    Args:
        answers (List[str]): The list of answers generated by the model.
        fluency_prompt_path (str): The path to the Finnish prompt template.
        correctness_prompt_path (str): The path to the correctness prompt template.
        model_name (str): The name of the model to use as the LLM judge.
        language (str): The language of the questions.
        finnish_questions_path (str): The path to the Finnish questions.
        client (mistralai.Mistral): The Mistral client.

    Returns:
        dict: The evaluation results.
    """
    fluency_prompt_template = load_prompt_template(fluency_prompt_path)
    correctness_prompt_template = load_prompt_template(correctness_prompt_path)
    truthfulqa = download_truthfulqa()
    questions = truthfulqa["question"]

    if language == "fi":
        with open(finnish_questions_path) as f:
            questions = json.load(f)["questions"]

    results = []
    for i, answer in enumerate(answers):
        logging.info(f"Evaluating generation {i}")
        truthfulqa_example = truthfulqa[i]
        question = questions[i]
        best_answer = truthfulqa_example["best_answer"]
        correct_answers = truthfulqa_example["correct_answers"]

        logging.info("Evaluating the fluency of the generation.")
        fluency_prompt = fluency_prompt_template.replace("<|QUESTION|>", str(question))
        fluency_prompt = fluency_prompt.replace("<|GENERATION|>", str(answer))
        generation_result = get_model_generation(client, fluency_prompt, model_name)
        fluency = extract_eval_results(generation_result)

        logging.info("Evaluating the correctness of the generation.")
        correctness_prompt = correctness_prompt_template.replace("<|QUESTION|>", str(question))
        correctness_prompt = correctness_prompt.replace("<|GENERATION|>", str(answer))
        correctness_prompt = correctness_prompt.replace("<|BEST_ANSWER|>", str(best_answer))
        correctness_prompt = correctness_prompt.replace("<|CORRECT_ANSWERS|>", str(correct_answers))
        generation_result  = get_model_generation(client, correctness_prompt, model_name)
        correctness = extract_eval_results(generation_result)
    
        results.append({"question": question, "best_answer": best_answer, "correct_answers": correct_answers, "generation": answer, "fluency": fluency, "correctness": correctness})
    
    return results