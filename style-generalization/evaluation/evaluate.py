from typing import List
import re
import logging
from datasets import load_dataset
import json

def extract_eval_results(generation_result: str):
    """Extracts the evaluation score from the LLM judge generation result.

    Args:
        generation_result (str): The text generated by the LLM judge.

    Returns:
        int or None: The evaluation score extracted from the generation result.
    """
    match = re.search(r"\[\[(.*?)\]\]", generation_result)
    if match:
        return match.group(1)
    else:
        return None

def get_model_generation(client, prompt: str, model_name: str):
    """Gets the model generation from the Mistral API.

    Args:
        client (mistralai.Mistral): The Mistral client.
        prompt (str): The prompt for text generation.
        model_name (str): The name of the model to use for generation.

    Returns:
        str: The generated text.
    """
    logging.info("Getting the model generation from the Mistral API.")
    chat_response = client.chat.complete(
        model=model_name,
        messages=[
            {
                "role": "user",
                "content": prompt
            },
        ],
        top_p=0.2,
        max_tokens=2000
    )
    generation_result = chat_response.choices[0].message.content
    return generation_result

def download_truthfulqa():
    """Downloads the TruthfulQA dataset from HuggingFace.

    Returns:
        dataset: The TruthfulQA dataset.
    """
    logging.info("Downloading the TruthfulQA questions from HuggingFace to get the questions.")
    truthfulqa = load_dataset("truthfulqa/truthful_qa", name="generation", split="validation")
    return truthfulqa["question"]

def load_prompt_template(prompt_path: str):
    """Loads the prompt template from the given path.

    Args:
        prompt_path (str): The path to the prompt template.

    Returns:
        str: The prompt template.
    """
    logging.info(f"Loading the prompt template from {prompt_path}.")
    with open(prompt_path, "r") as f:
        prompt_template = f.read()
    return prompt_template

def evaluate_model_generations(answers: List[str], baselines: List[str], comparison_prompt_path: str, fluency_prompt_path: str, model_name: str, client, language: str, finnish_questions_path: str):
    """Evaluates the model generations using the LLM judge.

    Args:
        answers (List[str]): The list of answers generated by the model using steering vectors.
        baselines (List[str]): The list of baseline answers generated without control applied.
        comparison_prompt_path (str): The path to the comparison prompt template.
        fluency_prompt_path (str): The path to the fluency prompt template.
        model_name (str): The name of the model to use as the LLM judge.
        client (mistralai.Mistral): The Mistral client.
        language (str): The language of the questions.
        finnish_questions_path (str): The path to the Finnish questions.

    Returns:
        dict: The evaluation results.
    """
    comparison_prompt_template = load_prompt_template(comparison_prompt_path)
    fluency_prompt_template = load_prompt_template(fluency_prompt_path)
    
    if language == "fi":
        with open(finnish_questions_path) as f:
            questions = json.load(f)["questions"]
    elif language == "en":
        questions = download_truthfulqa()

    results = []
    for i, answer in enumerate(answers):
        logging.info(f"Evaluating generation {i+1}")
        question = questions[i]
        baseline = baselines[i]

        logging.info("Comparing the generation with the baseline response.")
        comparison_prompt = comparison_prompt_template.replace("<|QUESTION|>", str(question))
        comparison_prompt = comparison_prompt.replace("<|GENERATION_A|>", str(answer))
        comparison_prompt = comparison_prompt.replace("<|GENERATION_B|>", str(baseline))
        generation_result = get_model_generation(client, comparison_prompt, model_name)
        eval1 = extract_eval_results(generation_result)

        logging.info("Comparing the generation with the baseline response - switched positions.")
        comparison_prompt = comparison_prompt_template.replace("<|QUESTION|>", str(question))
        comparison_prompt = comparison_prompt.replace("<|GENERATION_A|>", str(baseline))
        comparison_prompt = comparison_prompt.replace("<|GENERATION_B|>", str(answer))
        generation_result = get_model_generation(client, comparison_prompt, model_name)
        eval2 = extract_eval_results(generation_result)
        
        if eval1 == "A" and eval2 == "B":
            comparison_result = "win"
        elif eval1 == "B" and eval2 == "A":
            comparison_result = "lose"
        else:
            comparison_result = "tie"

        logging.info("Evaluating the fluency of the generation.")
        fluency_prompt = fluency_prompt_template.replace("<|GENERATION|>", str(answer))
        generation_result = get_model_generation(client, fluency_prompt, model_name)
        fluency_score = extract_eval_results(generation_result)
        
        results.append({
            "comparison_result": comparison_result,
            "fluency_score": fluency_score
        })

    return results